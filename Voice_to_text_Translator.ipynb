{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Voice-to-text Translator Backend**\n",
        "\n"
      ],
      "metadata": {
        "id": "GOW50tZ6G16Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading DataSet**"
      ],
      "metadata": {
        "id": "t6Ck8mcrJI-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O TED2020.en-ur.zip https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/en-ur.txt.zip\n",
        "!unzip TED2020.en-ur.zip\n"
      ],
      "metadata": {
        "id": "L25-2JFgG0_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_ted_dataset(en_file, ur_file):\n",
        "    with open(en_file, 'r', encoding='utf-8') as en, open(ur_file, 'r', encoding='utf-8') as ur:\n",
        "        english_sentences = en.readlines()\n",
        "        urdu_sentences = ur.readlines()\n",
        "\n",
        "    assert len(english_sentences) == len(urdu_sentences), \"Files line counts don't match.\"\n",
        "    data = {\"english\": english_sentences, \"urdu\": urdu_sentences}\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Updated file paths based on your screenshot\n",
        "en_file_path = \"TED2020.en-ur.en\"\n",
        "ur_file_path = \"TED2020.en-ur.ur\"\n",
        "\n",
        "# Load the dataset\n",
        "df = load_ted_dataset(en_file_path, ur_file_path)\n",
        "\n",
        "# Clean and save\n",
        "df['english'] = df['english'].str.strip()\n",
        "df['urdu'] = df['urdu'].str.strip()\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Save to a CSV file\n",
        "df.to_csv(\"ted_talks_english_urdu.csv\", index=False)\n",
        "\n",
        "print(\"Dataset preprocessing complete. Saved as 'ted_talks_english_urdu.csv'.\")"
      ],
      "metadata": {
        "id": "_HOBcXF_G0oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing relevant dependencies**"
      ],
      "metadata": {
        "id": "qKp2CODmI2-U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x1QyYdgLqDG"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn pyngrok whisper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "8hgCq_SHMM7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "id": "VeakrV7_Mi3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper"
      ],
      "metadata": {
        "id": "P3i68NUXM5ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-multipart"
      ],
      "metadata": {
        "id": "PEzMP0vUNiVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2opx3iwAs2jYTIWTw7r0pEyXevX_2YtqaJv3nMDqLk7Nq2i3K\n"
      ],
      "metadata": {
        "id": "OBAe8aYVO0Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y socat"
      ],
      "metadata": {
        "id": "Qae8YJGAz6jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi"
      ],
      "metadata": {
        "id": "ALl8hbNLJ39i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.0"
      ],
      "metadata": {
        "id": "vsIuJQxpQfNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "9wW6FKPRQomX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model training and data preprocessing**"
      ],
      "metadata": {
        "id": "_Yl4vzIRpG5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from datasets import load_dataset\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "import os\n",
        "\n",
        "# Disable WANDB for now\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Check if CUDA (GPU) is available and use it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset('csv', data_files={\"train\": \"ted_talks_english_urdu.csv\"})\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "# Ensure no None or empty values\n",
        "dataset = dataset.filter(lambda example: example['english'] and example['urdu'])\n",
        "\n",
        "# Load Pretrained MarianMT model and tokenizer\n",
        "model_name = \"abdulwaheed1/english-to-urdu-translation-mbart\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"ur_PK\", tgt_lang=\"en_XX\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)  # Move model to GPU if available\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [text for text in examples['urdu']]  # Urdu as source\n",
        "    targets = [text for text in examples['english']]  # English as target\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=150, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=150, truncation=True, padding=\"max_length\").input_ids\n",
        "\n",
        "    labels = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    save_total_limit=2,\n",
        "    generation_max_length=150,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    # Use mixed precision for faster training\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./urdu_to_english_finetuned_model\")\n",
        "tokenizer.save_pretrained(\"./urdu_to_english_finetuned_model\")\n"
      ],
      "metadata": {
        "id": "iCLxWzIIpB9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from datasets import load_dataset\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "import os\n",
        "\n",
        "# Disable WANDB for now\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Check if CUDA (GPU) is available and use it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset('csv', data_files={\"train\": \"ted_talks_english_urdu.csv\"})\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "# Ensure no None or empty values\n",
        "dataset = dataset.filter(lambda example: example['english'] and example['urdu'])\n",
        "\n",
        "# Load Pretrained MarianMT model and tokenizer\n",
        "model_name = \"abdulwaheed1/english-to-urdu-translation-mbart\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"en_XX\", tgt_lang=\"ur_PK\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)  # Move model to GPU if available\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Prepare the translation inputs for the MarianMT model\n",
        "    inputs = [text for text in examples['english']]\n",
        "    targets = [text for text in examples['urdu']]\n",
        "\n",
        "    # Tokenize inputs and labels with padding\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
        "\n",
        "    # Replace padding token ID for labels with -100 (ignored in loss calculation)\n",
        "    labels = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    save_total_limit=2,\n",
        "    generation_max_length=128,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    # Use mixed precision for faster training\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./ted_talks_finetuned_model\")\n",
        "tokenizer.save_pretrained(\"./ted_talks_finetuned_model\")"
      ],
      "metadata": {
        "id": "QS0Un5keo-uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading model from google drive**"
      ],
      "metadata": {
        "id": "Jn07OTGYJf97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "w3mMYf5HNeSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "9UOvDMvUNldk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/NLP_Translator_modle/ted_talks_finetuned_model.zip /content/"
      ],
      "metadata": {
        "id": "HvTh_DbcNuFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ted_talks_finetuned_model.zip"
      ],
      "metadata": {
        "id": "wCwn03X_Nyfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/NLP_Translator_modle/urdu_to_english_finetuned_model.zip /content/"
      ],
      "metadata": {
        "id": "Zi5t6baakbt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip urdu_to_english_finetuned_model.zip"
      ],
      "metadata": {
        "id": "Uss5VfdhlBgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evalution**"
      ],
      "metadata": {
        "id": "G4TQdhcaSzBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "yJ4DS3ZBf4rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "4F91EKTXgUp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "from evaluate import load\n",
        "from sklearn.metrics import f1_score\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load TED Talks dataset\n",
        "dataset = load_dataset('csv', data_files={\"train\": \"ted_talks_english_urdu.csv\"})\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "test_dataset = dataset['test']  # Use the test split for evaluation\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_name = \"./urdu_to_english_finetuned_model\"  # Path to your model\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"ur_PK\", tgt_lang=\"en_XX\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Initialize BLEU score and F1 score metrics\n",
        "bleu_metric = load(\"bleu\")\n",
        "\n",
        "# Function to translate Urdu to English\n",
        "def translate_urdu_to_english(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=150, truncation=True, padding=\"max_length\")\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move tensors to the correct device\n",
        "    outputs = model.generate(**inputs, max_length=150, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Limit the number of samples to evaluate\n",
        "sample_size = 1\n",
        "df_sampled = test_dataset.select(range(sample_size))  # Select first `sample_size` samples\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "# Iterate over the sampled test data\n",
        "for row in df_sampled:\n",
        "    urdu_text = row['urdu']\n",
        "    reference_translation = row['english']\n",
        "\n",
        "    # Skip rows with missing or empty Urdu text\n",
        "    if not isinstance(urdu_text, str) or not urdu_text.strip():\n",
        "        continue\n",
        "\n",
        "    # Translate and store predictions and references\n",
        "    prediction = translate_urdu_to_english(urdu_text)\n",
        "    predictions.append(prediction)\n",
        "    references.append([reference_translation])  # BLEU metric expects a list of references\n",
        "\n",
        "\n",
        "# Ensure predictions and references are of the same length\n",
        "assert len(predictions) == len(references), f\"Length mismatch: {len(predictions)} != {len(references)}\"\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = bleu_metric.compute(predictions=predictions, references=references)\n",
        "print(f\"BLEU Score: {bleu_score['bleu']}\")\n",
        "\n",
        "# Convert predictions and references to tokens for F1 score calculation\n",
        "def tokenize_sentences(sentences):\n",
        "    return [sentence.split() for sentence in sentences]\n",
        "\n",
        "tokenized_predictions = tokenize_sentences(predictions)\n",
        "tokenized_references = tokenize_sentences([ref[0] for ref in references])\n",
        "\n",
        "# Flatten the tokenized lists\n",
        "flat_predictions = [item for sublist in tokenized_predictions for item in sublist]\n",
        "flat_references = [item for sublist in tokenized_references for item in sublist]\n",
        "\n",
        "# Calculate F1 score (micro average)\n",
        "f1 = f1_score(flat_references, flat_predictions, average='micro')\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "FvRo9noCSfF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**API**"
      ],
      "metadata": {
        "id": "0XDKeLTbAjLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from fastapi.responses import JSONResponse\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pydub import AudioSegment\n",
        "import whisper\n",
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "\n",
        "# Apply the nest_asyncio patch\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "whisper_model = whisper.load_model(\"medium\")\n",
        "\n",
        "# Load the fine-tuned models and tokenizers\n",
        "# English to Urdu\n",
        "model_path_eng_to_urd = \"./ted_talks_finetuned_model\"\n",
        "tokenizer_eng_to_urd = MBart50TokenizerFast.from_pretrained(model_path_eng_to_urd, src_lang=\"en_XX\", tgt_lang=\"ur_PK\")\n",
        "model_eng_to_urd = MBartForConditionalGeneration.from_pretrained(model_path_eng_to_urd)\n",
        "\n",
        "# Urdu to English\n",
        "model_path_urd_to_eng = \"./urdu_to_english_finetuned_model\"\n",
        "tokenizer_urd_to_eng = MBart50TokenizerFast.from_pretrained(model_path_urd_to_eng, src_lang=\"ur_PK\", tgt_lang=\"en_XX\")\n",
        "model_urd_to_eng = MBartForConditionalGeneration.from_pretrained(model_path_urd_to_eng)\n",
        "\n",
        "# Move models to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_eng_to_urd.to(device)\n",
        "model_urd_to_eng.to(device)\n",
        "\n",
        "def translate_text(text, direction):\n",
        "    \"\"\"Handles translation based on the specified direction.\"\"\"\n",
        "    if direction == \"eng-to-urd\":\n",
        "        tokenizer, model = tokenizer_eng_to_urd, model_eng_to_urd\n",
        "    elif direction == \"urd-to-eng\":\n",
        "        tokenizer, model = tokenizer_urd_to_eng, model_urd_to_eng\n",
        "    else:\n",
        "        raise ValueError(\"Invalid translation direction!\")\n",
        "\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=150, truncation=True, padding=\"max_length\")\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move tensors to the correct device\n",
        "\n",
        "    # Generate translation\n",
        "    outputs = model.generate(**inputs, max_length=150, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Configure CORS middleware to accept requests from any origin\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Allows all origins; specify frontend URL if needed\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],  # Allows all methods (GET, POST, etc.)\n",
        "    allow_headers=[\"*\"],  # Allows all headers\n",
        ")\n",
        "\n",
        "# Create a tunnel to the FastAPI app\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"FastAPI is accessible at:\", public_url)\n",
        "\n",
        "# Define a Pydantic model to handle the message data\n",
        "class Message(BaseModel):\n",
        "    message: str\n",
        "\n",
        "# Root endpoint (for testing)\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Welcome to the FastAPI app!\"}\n",
        "\n",
        "# Hello World endpoint (for testing)\n",
        "@app.get(\"/hello\")\n",
        "async def hello_world():\n",
        "    return {\"message\": \"Hello World\"}\n",
        "\n",
        "# Favicon endpoint (to avoid 404 error for GET /favicon.ico)\n",
        "@app.get(\"/favicon.ico\")\n",
        "async def favicon():\n",
        "    return JSONResponse(content={})\n",
        "\n",
        "# Endpoint to accept the message from the frontend via POST\n",
        "@app.post(\"/send-message\")\n",
        "async def send_message(data: Message):\n",
        "    return {\"received_message\": data.message}\n",
        "\n",
        "# Endpoint to upload audio data and transcribe with Whisper\n",
        "@app.post(\"/upload-audio\")\n",
        "async def upload_audio(request: Request):\n",
        "    try:\n",
        "        audio_data = await request.body()  # Read binary data\n",
        "        direction = request.headers.get(\"Translation-Direction\", None)\n",
        "\n",
        "        if not direction or direction not in [\"eng-to-urd\", \"urd-to-eng\"]:\n",
        "            return JSONResponse(\n",
        "                content={\"error\": \"Invalid or missing Translation-Direction header\"},\n",
        "                status_code=400\n",
        "            )\n",
        "\n",
        "        # Save the audio data temporarily as an Opus file\n",
        "        opus_path = \"temp_audio.opus\"\n",
        "        with open(opus_path, \"wb\") as f:\n",
        "            f.write(audio_data)\n",
        "\n",
        "        # Define the WAV file path\n",
        "        wav_path = \"temp_audio.wav\"\n",
        "\n",
        "        # Convert Opus to WAV using ffmpeg command line\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"ffmpeg\", \"-y\", \"-i\", opus_path, wav_path],\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE\n",
        "            )\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            error_message = e.stderr.decode()\n",
        "            print(f\"FFmpeg error: {error_message}\")\n",
        "            return JSONResponse(\n",
        "                content={\"error\": \"Audio conversion failed\", \"details\": error_message},\n",
        "                status_code=500\n",
        "            )\n",
        "\n",
        "        # Transcribe the audio using the local Whisper model\n",
        "        try:\n",
        "          if direction == \"eng-to-urd\":\n",
        "            result = whisper_model.transcribe(wav_path)\n",
        "            transcription = result[\"text\"]\n",
        "          else:\n",
        "            result = whisper_model.transcribe(wav_path, language=\"Urdu\")\n",
        "            transcription = result[\"text\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Whisper transcription error: {str(e)}\")\n",
        "            return JSONResponse(\n",
        "                content={\"error\": \"Transcription failed\", \"details\": str(e)},\n",
        "                status_code=500\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            translation = translate_text(transcription, direction)\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {str(e)}\")\n",
        "            translation = \"Translation unavailable\"\n",
        "\n",
        "\n",
        "        # Cleanup temporary files\n",
        "        os.remove(opus_path)\n",
        "        os.remove(wav_path)\n",
        "\n",
        "        # Return the transcription result\n",
        "        return {\"message\": \"Audio received\", \"transcription\": transcription, \"translation\" : translation}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"General error: {str(e)}\")\n",
        "        return JSONResponse(\n",
        "            content={\"error\": \"Unexpected error occurred\", \"details\": str(e)},\n",
        "            status_code=500\n",
        "        )\n",
        "\n",
        "# To run the server with FastAPI in Jupyter/Colab\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=5000)s\n"
      ],
      "metadata": {
        "id": "1Zr7RpS-CqIN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}